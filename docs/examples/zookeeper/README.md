# Tutorial - Falcon with Zookeeper

This tutorial shows how one can use Falcon to analyze an execution of [Apache Zookeeper](https://zookeeper.apache.org). The tutorial is divided into the three steps, corresponding to the three phases of the Falcon's workflow pipeline.

**Requirements:**

- Kernel v4.8+
- [Apache Zookeeper (3.5.0-alpha)](https://zookeeper.apache.org/doc/r3.5.0-alpha/zookeeperStarted.html)

## Initial Setup

For this experiment, we need to setup two Zookeeper servers and a Zookeeper client that performs a couple of operations against servers.

### Server

- Configure two distinct Zookeeper servers (pseudo-distributed or distributed mode).

- For recording _LOG_ events with the Falcon's pipeline expected format, copy the `falcon-log4j-appender.jar` file to the `lib` folder of your Zookeeper directory and add the following appender to your `conf/log4j.properties`.

```ini
#
# Add FALCONFILE to rootLogger to get log file output
#   Log events for Falcon pipeline
#
log4j.appender.FALCONFILE=org.apache.log4j.FileAppender
log4j.appender.FALCONFILE.File=zookeeker_falcon.log
log4j.appender.FALCONFILE.layout=org.haslab.jni.FalconPatternLayout
log4j.appender.FALCONFILE.layout.ConversionPattern={"timestamp":%e, "thread": "%T@%h", "type": "LOG", "message": "%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m", "data":{"level": "%p"}}%n
```

### Client

For this experiment, we developed a Java client that executes a couple of operations against Zookeeper servers. It is available under the `zookeeper-client` folder.

```
cd zookeeper-client && mvn package
```

As we've done for servers, we need to copy the `falcon-log4j-appender.jar` file to the `target/lib/` folder, in order to record _LOG_ events coming from the client with the expected format.

```
cp falcon-log4j-appender.jar zookeeper-client/target/lib
```

## Event Tracing

After the setup of Zookeeper servers and the client, we will start to trace their execution with _falcon-tracer_.

For each server, go to the Zookeeper server's folder and run the following command to start it within the _falcon-tracer_.

```bash
# Start Zookeeper server with the Falcon appender
export ZOO_LOG_DIR=. && \
export ZOO_LOG4J_PROP='INFO,CONSOLE,FALCONFILE' && \
sudo falcon-tracer bin/zkServer.sh start-foreground
```

At this point, the servers are running and being traced by the Falcon's tracer. Now, it is time to start the `zookeeper-client` client. Please note that the following command needs the IP of one of the running servers.

```bash
sudo falcon-tracer java -jar target/zookeeper-client-example-1.0-SNAPSHOT.jar <server_ip>
```

### Collecting traces

Once you stop the execution of both servers and the client, we need to collect the recorded trace of each of them.

The full trace is split into two kinds of files for each running entity: one with _LOG_ events, resulting from the Falcon's Log4j appender, and the other, generated by _falcon-tracer_, with all of the events related to the operating system level (OS level).

Each log resulting from Log4j (client and each server) is placed in the directory that you've set in the `log4j.appender.FALCONFILE.File` configuration property. If you didn't change the appender's configuration, then it is expected to be found in the current working directory, _i.e._, the directory in which the start command was executed.

Regarding the trace file with OS-level events, similarly to the unchanged Log4j, it is also placed in the current working directory.

If you used several nodes to setup this experiment, please be aware that you will need to collect them into a single local folder for further processing. To this end, you can use commands like `scp` and `rsync`.

You are expected to end up with something similar to the `zktrace` folder available in this example.

## Causality Inference

After running Zookeeper and collecting the components' traces, we need to combine them into a single execution event trace.

```bash
cat zktrace/*.log > zktrace_full.log
```

Note that, at this point, the full event trace is simply a concatenation of each component's log without any causal order established across the events. To obtain a coherent execution trace, we rely on *falcon-solver* to infer the happens-before relationships.

```bash
java -jar falcon-solver/target/falcon-solver-1.0-SNAPSHOT-jar-with-dependencies.jar --event-file zktrace_full.log --output-file zktrace_full_ordered.log
```

If everything goes as expected, falcon-solver should yield the causally-ordered execution trace after some seconds. Falcon-solver also outputs some statistics about the constraint solving procedure such as the number of constraints in the model and the solving time.

## Diagram Visualization

The last step of Falcon's pipeline consists of generating and visualizing the space-time diagram of the execution. This is done by, first, starting the falcon-visualizer:

```bash
cd falcon-visualizer
npm run dev
```

After the browser opens, click the button *Choose File* and select file `zktrace_full_ordered.log`. Then, click the button *Tick* to start drawing the diagram. By systematically clicking the button (or pressing the space bar), new logical timestamps will appear on the left side of the screen and the events occurring at those timestamps will be drawn along with their causal dependencies.
